### 上午

- ~~🕗起床，吃了一个昨天在小高栗子买的面包，不是很好吃。起来后听了一个很简短的听力，内容是一种关于担心朋友在玩乐却不叫自己的心态，具体名字是FOMO，下面回忆一下单词：~~

  **<u>FOMO  fear of missing out 社交控</u> 	<u>acronym 首字母缩写</u> 	<u>moan 呻吟，抱怨</u>** 

### 下午

- 下午对之前看过的cs231n中关于在深度学习中使用的软件部分进行了复习回顾：

#### CPU、GPU和TPU 

**CPU运算能力强**，但是**核心相对较少**，处理数据只能**串行**进行。由于CPU每次进行运算都要先对内存进行访问，所以**内存访问成为CPU构架的不足**。

**GPU**虽然单个核心运算能力不如CPU，但是**核心数目多**，可以**并行计算**，对于矩阵的运算过程较CPU要快得多，现在有很多深度适用于GPU~~的深度学习~~**编程框架**，比较有名的是**CUDA(只用于NVIDIA)**和**OpenCl(用于所有)**，在进行**深度学习中使用的GPU大多是NVIDIA**的。GPU拥有的上千个ALU使得它可以同时进行数千次乘法和加法运算，但是其**同样需要访问内存来存储和获取中间数据，物理空间占用量大**。

**TPU**是**Tensor Process Unit**的缩写，由谷歌发明，主要是用于深度学习，速度最快。TPU专门处理神经网络，其**独特的脉动阵列将众多加法器和乘法器连接在一起，在整个计算过程中不需要再次访问内存，所以速度最快**。

[What makes TPUs fine-tuned for deep learning?]: https://cloud.google.com/blog/products/ai-machine-learning/what-makes-tpus-fine-tuned-for-deep-learning

#### 深度学习框架

很多公司都开发了深度学习框架，比较有名的是**Google的TensorFlow和Facebook的Pytorch**，都可以用GPU进行运算

- **Tensorflow**是一种**静态图**模型，首先需要绘制计算图，然后再进行运算，其中还有很多**集成度很高的包**可以使用，例如**Keras**、**TFlearn**等等
- **PyTorch**是一种**动态图**模型，其中**高度集成API Torch.nn**用起来十分方便，**动态图模型也经常用于RNN**
- **Numpy不能在GPU上运行**

#### 一道算法题

> 根据 百度百科 ，生命游戏，简称为生命，是英国数学家约翰·何顿·康威在 1970 年发明的细胞自动机。
>
> 给定一个包含 m × n 个格子的面板，每一个格子都可以看成是一个细胞。每个细胞都具有一个初始状态：1 即为活细胞（live），或 0 即为死细胞（dead）。每个细胞与其八个相邻位置（水平，垂直，对角线）的细胞都遵循以下四条生存定律：
>
> 如果活细胞周围八个位置的活细胞数少于两个，则该位置活细胞死亡；
> 如果活细胞周围八个位置有两个或三个活细胞，则该位置活细胞仍然存活；
> 如果活细胞周围八个位置有超过三个活细胞，则该位置活细胞死亡；
> 如果死细胞周围正好有三个活细胞，则该位置死细胞复活；
> 根据当前状态，写一个函数来计算面板上所有细胞的下一个（一次更新后的）状态。下一个状态是通过将上述规则同时应用于当前状态下的每个细胞所形成的，其中细胞的出生和死亡是同时发生的。
>
> 示例：
>
> 输入： 
> [
>   [0,1,0],
>   [0,0,1],
>   [1,1,1],
>   [0,0,0]
> ]
> 输出：
> [
>   [0,0,0],
>   [1,0,1],
>   [0,1,1],
>   [0,1,0]
> ]

本题可以通过卷积的方式进行求解，设置一个3*3的卷积核，中间数为0，对原输入列表进行卷积操作后输出的值就是当前位置的相邻细胞数，再有细胞数判断当前细胞的状态

### 晚上

- 晚上首先看了两集白板ML，主要讲述为什么要进行降维，介绍了维度灾难：

![image-20200402221331979](/Users/lianxing/Library/Application Support/typora-user-images/image-20200402221331979.png)

当维度很高时，原本的中间圆形在高维空间形成的超立方体的体积会趋向于0: $V_{超立方体}=k\times0.5^D$，当$D$很大时，此时超立方体的体积为0，这和我们在低维空间的常识不同。在高维空间中需要的数据量成指数级增加，根据上面的图示我们可以直观想象在低维空间中能够填满维度的样本在高维空间中只是分布在表面

只后对讲解PCA之前的预备知识进行了介绍：将协方差的表达形式写成矩阵的形式

![image-20200402222937538](/Users/lianxing/Library/Application Support/typora-user-images/image-20200402222937538.png)

其中H表示中心化矩阵，其代表的含义是将数据向中心位移

- [x] 三个小时
- [x] 五个小时
- [x] 一天
- [ ] 三天
- [x] 五天
- [x] 十天
- [ ] 一个月