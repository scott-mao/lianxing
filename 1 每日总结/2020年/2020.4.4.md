4月3号玩了一天，就啥也没干，不过搞了个花里胡哨的桌面

![image-20200404221235696](/Users/lianxing/Library/Application Support/typora-user-images/image-20200404221235696.png)

### 上午

- 上午啥也没干

### 下午

- 下午对增强学习的第一课进行了总结，了解了增强学习的过程：**agent通过与环境互动达到执行使得累积reward最大的action**

  学习了增强学习与其他学习方式的区别：增强学习学习的是一个过程，通过与环境及交互学习能够使得积累奖赏最大的行为

  增强学习中面临的四个挑战：

  **1.optimize 优化**

  对于特定的state如何优化得到使得最终积累奖赏最大的action

  **2.generalize泛化**

  如何对没有见到过的state反应action

  **3.delayed consequence 延迟后果**

  过程结束后才能获得reward，在最后才能得到的奖赏如何分配到之前的各个action中去

  **4.exploration探索**

  如何对没有经历过的state进行探索，是保持当前的action过程还是探索新的action

  > 举个例子
  >
  > 监督学习和无监督学习都是具有优化和泛化过程，但是由于直接得到最终结果并且有一定的方程规范所以没有延迟后果和探索
  >
  > 模仿学习有优化、泛化和延迟后果，因为他通过模仿一种经历去学习，经历最终会有结果，从这个结果中去学习分配不同的action

- 学习了增强学习中一些基本概念

  顺序决策过程：决策过程顺序发生

  马尔科夫特性：下一状仅与当前状态有关

  一个agent的组成：

  1.policy 状态到行为的映射关系

  2.value fuction 奖赏值之和，对之后的奖赏值加了衰减系数![image-20200404233215105](/Users/lianxing/Library/Application Support/typora-user-images/image-20200404233215105.png)

  3.model 模型 一个agent可以有一个表示其余环境互动过程的模型，称为model-based，如果没有相应的模型而只于真实环境进行互动称为model-free 

- **value-based agent 基于价值的agent** 

- **policy-based agent 基于概率的agent**

- [x] 当天
- [ ] 一天
- [x] 三天
- [x] 五天
- [x] 十天
- [ ] 一个月