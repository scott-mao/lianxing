# 弱监督目标定位论文整理

## 1.Unveiling the Potential of Structure Preserving for Weakly Supervised Object Localization

### 1简介

这篇是CVPR2021的文章，这篇文章提出的方法主要运用了热力图的空间相关性，利用feature map的自相关性以及热力图获得的目标最具判别力的区域得到目标整体。

### 2主要方法

作者将其命名为Structure-Preserving Activation，主要包括两个模块，分别是Restricted Activation Module和Self-correlation Map，前一个模块直接使用热力图中的信息通过阈值限制生成背景无物体部分，后一个模块利用热力图的自相关性学习空间相关性的信息再次得到背景和目标部分。

#### 2-1 Restricted Activation Module

![image-20210406215024366](image-20210406215024366.png)

在获得热力图CAM之后通过阈值选取得到背景$M_{bg}$和目标区域$M_{obj}$

![image-20210406215106836](image-20210406215106836.png)

![image-20210406215116231](image-20210406215116231.png)

然后和原始的CAM图做像素级别的loss：

![image-20210406215337542](image-20210406215337542.png)

这部分能够使得热力图更多的关注于目标部分。

#### 2-2 Self-correlation Map Generating

![image-20210406215437935](image-20210406215437935.png)

这部分使用CAM的自相关图学习与目标区域以及背景区域在空间上相似的部分。

- 生成自相关图

一阶自相关：首先需要生成自相关图，自相关图是衡量feature map像素级别的相似度，其中$f\in R^{HW\times C}$就是生成的feature map，然后单位化后与其转置相再经过ReLU激活后得到的相关性$SC(f)\in R^{HW\times HW}$，这里得到的就是每一个像素和其他像素的相似程度：

![image-20210406215710056](image-20210406215710056.png)

二阶自相关：二阶是在一阶像素相乘计算基础上再次相乘得到的，实际上衡量的是一阶自相关图的自相关性

![image-20210406220317794](image-20210406220317794.png)

在一阶和二阶自相关之后将两者结合就得到高阶自相关图（high-order self-correlation (HSC) ）

![image-20210406221136205](image-20210406221136205.png)

- 得到关注区域，生成特定相关图

通过CAM得到关注区域后就可以将之结合自相关图得到与关注部分像素级别相关的区域，结合之前得到的自相关图以及像素区域得到在HSC下的目标区域：

![image-20210406221145460](image-20210406221145460.png)

由于该目标区域的像素值不是唯一的，所以这个时候对所有该区域像素的相关图取平均值得到物体部分的mask：

![image-20210406221435564](image-20210406221435564.png)

同样的道理也可以获得背景区域的自相关图以及对应的mask，然后通过作差得到最终的关注部分：

![image-20210406221614414](image-20210406221614414.png)

### 3主要创新

利用阈值探索背景以及目标区域的手段在SPG中已经有所体现，该论文的主要创新之处在于结合CAM使用的自相关图获取到空间像素级别的相关信息。

### 4 想法

- 这种像素级别相关性的获取是在最后一层feature map上面，能否在结合原图的像素相关性
- 像素级别的空间相关性显然对于具有同一颜色或者纹理的物体十分适用，但当物体的部分具有明显的不连续区域以及较大差别的时候这种方法适用效果可能较差，如何获取这部分物体的相关特征呢？
- 这里只是使用了一张图片中自身的feature map自相关性，能否结合同类图片的进行相关性分析

- [ ] 论文方法复现

## 2. In-sample Contrastive Learning and Consistent Attention for Weakly Supervised Object Localization

### 1 简介

ACCV2020的文章，有点坑的是现在代码都没有放出来。

这篇文章主要用到了四个部分

1. non-local attention block，实际上就是一种self-attention的机制
2. contrastive attention loss，模仿对比学习生成的attention的loss，这部分主要用于对背景部分进行学习以及加强已学习到的目标区域
3. foreground consistency loss，用于对不同阶段学习到的feature map进行统一，保持其在特征提取上的一致性
4. random selection，随机选取重要的掩膜与原来的feature map进行点乘，这部分思想有点借鉴ADL

###2 主要方法

按照上面说的对四个部分进行简要介绍：

#### 2.1 non-local attention block

这部分实际上就是自注意力机制的运用，详情可以借鉴深度学习-常用Trick-Attention机制部分的讲解，这里贴一张Non-local这篇文章中的图片来更好理解其主要结构

![image-20210407221551375](image-20210407221551375.png)

该文模仿上面的结构运用的non-local block如下：

![image-20210407221624404](image-20210407221624404.png)

#### 2.2 contrastive attention loss

发现作者很会起名，或者说很会借鉴别的文章。借鉴对比学习，文中对attention map按照阈值分为背景图、前景图以及去除最明显特征的前景图，将这几部分作为掩膜与原来的feature map进行点乘后映射到特征空间，最后通过contrastive attention loss使得前景图部分在隐空间接近而前景和背景部分分离。下面这张图总结的很不错：

![image-20210407222253994](image-20210407222253994.png)

具体的生成部分公式如下：

![image-20210407222359215](image-20210407222359215.png)

映射到隐空间后使用L2距离衡量相似度。

####2.3 foreground consistency loss

这部分主要是为了保证前后提取特征的一致性，对前后feature map直接用L2距离衡量相似度

![image-20210407222745948](image-20210407222745948.png)

####2.4 random selection

这部分显示就是借鉴了ADL的方法，对于feature map进行随机增强，实际上有点正则化的意味在里面。

![image-20210407223146790](image-20210407223146790.png)

### 3 主要创新

这篇文章作者可以说是读了很多相关文献，方法很多都是借鉴来修改得到的，但是这两个loss的设置的确很有说服力，但是看文章提供的实验结果实际上并没有多么惊艳，在新的指标MaxBoxAccV2上提升并不大，而且文章还没有给出代码复现，总感觉实验结果不是上面说的那样。

### 相关文章

- [ ] 对比学习（contrastive learning）

1. He,K.,Fan,H.,Wu,Y.,Xie,S.,Girshick,R.:Momentumcontrastforunsupervised visual representation learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. (2020) 9729–9738
2. Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for con- trastive learning of visual representations. arXiv preprint arXiv:2002.05709 (2020)

- [ ] Non-local结构

1. Wang, X., Girshick, R., Gupta, A., He, K.: Non-local neural networks. In: Proceed- ings of the IEEE conference on computer vision and pattern recognition. (2018)

   7794–7803