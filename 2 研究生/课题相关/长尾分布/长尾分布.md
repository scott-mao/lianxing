# 长尾分布

## 1.概念

在我们实际获得的数据集中往往会存在某一类别的样本数量较多，而某些类别的样本数量较少的情况，造成这种现象的原因可能是某个类别的图片很难获取，在这种情况下数据集中各个类别的图片就存在数量不同的现象，我们称这种数据集存在长尾分布。

![img](https://pic3.zhimg.com/80/v2-3c2009cd25376e7bd63b40cee7aa3de6_720w.jpg)

研究意义：

- 很多样本图片不好获取或者存在人工标记困难的情况，如果模型能够在具有长尾分布的数据集中表现良好的话就能减少很多图片的获取以及标注成本。

  

## 2.常用方法

对于具有长尾分布的数据集一种很自然的想法就是对那些数量较少类别的图片进行扩充，通过现有的数据增强的方案以及可能的深度学习模型(例如GAN)可以对图片进行扩充，这类方法也称为图像增广；另外一种想法就是在训练过程中改变对于不同样本的采样频率使得那些数量较少的样本更多的被模型训练学习，这就改变了对于不同类别图片的采样概率，这类方法称为重采样；还有一种方法就是改变在模型训练过程中产生的loss，对于那些类别较少的样本的loss给予一个较大的权重，使得模型更倾向于学习这些样本较少的类别，这类方法称为重加权。

###2.1 重采样(re-sampling)

重采样的方式改变了不同类别图片被采样到的概率，很直观的想法就是提高那些那些类别较少的图片被采样的可能性从而最终实现各个类别图片采样到的数量均衡，其中最常用的方法是类别均衡采样的方式（class-balanced sampling）,这种方法核心思想是根据不同样本的数量分配不同的被采样的可能性，引用Decoupling Representation (ICLR 2020)[1]的通用公式：
$$
p_j=\frac{n^q_j}{\sum_{i=1}^Cn_i^q}
$$
其中 $q$ 的值选择为1的时候每个类样本都具有相同的概率被采样，当 $q\in[0,1)$ 的时候相当于对数量少类别增加了被采样到的概率。在这种方法下尾部数据（数量少类别数据）可能被重复采样到，这个时候可以使用一些数据增强的方式。

- 缺点与优点：重采样通过人为改变采样概率的方式使得模型训练过程中的数据均衡，能够减少模型对头部数据的过拟合。但由于尾部数据往往较少，缺少样本之间的差异从而导致模型出现鲁棒性不高的现象，同时头部数据可能存在大量差异特征也不能得到完全学习。

###**相关文章**

- [ ] 1.**Decoupling Representation and Classifier for Long-Tailed Recognition, ICLR 2020**
- [ ] 实验

这篇文章指出特征提取部分和分类器部分的学习应该分别进行，对于特征提取的backbone使用原始数据进行训练（长尾分布的数据），而对于分类器部分则使用重采样后数据进行训练。具体的做法是首先使用原始数据对模型进行训练，然后固定住backbone部分的参数后使用重采样方式再次对分类器部分进行训练。

- [ ] 2.**BBN: Bilateral-Branch Network with Cumulative Learning for Long-Tailed Visual Recognition，CVPR 2020**
- [ ] 实验

类似与上面文章的思想，这篇文章训练了一个双分支网络，分别使用原始数据和重采样进行学习然后对两个分支动态加权。

- [ ] 3.**Dynamic Curriculum Learning for Imbalanced Data Classification，ICCV 2019**







### 2.2 重加权（re-weighting）

重加权的方式作用与训练过程中模型产生的loss值，对于尾部数据产生的loss进行更大的加权，而相对于头部数据则加权系数较小。类似的思想也出现在很多目标检测和实例分割等任务中，例如Focal loss等方法。一个重加权的交叉熵损失的通用公式是：

![[公式]](https://www.zhihu.com/equation?tex=loss%3D-%5Cbeta+%5Ccdot+log+%5Cfrac%7Bexp%28z_j%29%7D%7B%5Csum_%7Bi%3D1%7D%5EC+exp%28z_i%29%7D)

其中 $z_j$ 是网络输出的logit，经过softmax函数后变为概率，重加权中的权重就是 $\beta$ ，他的取值根据要计算样本的不同而不同，即给头部数据更低的权重，而给尾部数据更高的权重以反向抵消长尾效应。一种比较简单的实现方式是：

![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta%3Dg%28%5Cfrac%7B%5Csum_%7Bi%3D1%7D%5EC+f%28n_i%29%7D%7Bf%28n_j%29%7D%29)

其中函数 $f(),g()$ 是任意单调函数。

### **相关文章**

- [ ] 1.**Class-Balanced Loss Based on Effective Number of Samples，CVPR 2019**

这篇文章找到一个更好的设计重采样权重的方式

- [ ] 2.**Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss，NIPS 2019**



参考文章：

[Long-Tailed Classification (1) 长尾(不均衡)分布下的分类问题简介](https://zhuanlan.zhihu.com/p/153483585)

[Long-Tailed Classification (2) 长尾分布下分类问题的最新研究]()


















