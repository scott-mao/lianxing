# 自动调参

在深度学习的模型中往往存在很多超参数，如何进行超参数的选择呢？手工调参的方法显然过于低效，可以首先对超参数的取值划定范围然后使用自动选择的方式对各个超参数进行选择后验证其效果，具体的方法有：

- Grid_search 对参数划定范围和步长，对其中每个参数都进行选择然后实验其效果，优点是简单暴力，搜索范围较广结果比较可靠，缺点就是很浪费时间，当存在大量超参数需要选择的时候需要大量运行时间。
- Random_search 随机搜索的方式从一定范围内对参数进行随机选择，这种方式需要的时间更少，更加高效。
- Bayesian_optimization 贝叶斯优化的方式（没学习）

## 具体应用code

这里使用自动调参工具optuna，通过一个简单的例子来解释其应用：

```python
import optuna

def objective(trial):
    x = trial.suggest_uniform('x', -10, 10)#一次选取需要优化的参数，括号内x为该参数在学习过程中的记录形式
    target = (x-2)**2 #目标函数
    return target

study = optuna.create_study()
study.optimize(objective, n_trials=100)

study.best_params  # E.g. {'x': 2.002108042}

```

在使用 optuna 的时候需要自己构建一个需要优化的目标 `objective(trail) `，然后就直接调用 optuna 中的函数进行学习即可。在这里例子中：

1. 在 `objective(trail)` 中 trial 指的是一次尝试，其通过调用 `sugget_uniform` 来对参数进行一次选择，这里 `x` 就是我们需要优化的参数，得到参数后我们需要表达出优化的目标，该目标值应当可以被优化参数影响，并且可以量化比较。在本例中为 `target = (x-2)**2` 。
2. 要使用 optuna 进行参数优化首先需要构建一个 `study` 对象 `optuna.create_study()` ，然后使用其对应函数 `optimize` ，传入参数就是需要优化的对象以及选择的次数。

## Optuna简介

简单介绍一下optuna里最重要的几个term，想直接看在PyTorch里咋用可以直接跳过。

**1）在optuna里最重要的三个term**：

（1）Trial：对objective函数的一次调用；

（2）Study：一个优化超参的session，由一系列的trials组成；

（3）Parameter：需要优化的超参；

在optuna里，study对象用来管理对超参的优化，optuna.create_study()返回一个study对象。

**2）study又有很多有用的property**：

（1）study.best_params：搜出来的最优超参；

（2）study.best_value：最优超参下，objective函数返回的值 (如最高的Acc，最低的Error rate等)；

（3）study.best_trial：最优超参对应的trial，有一些时间、超参、trial编号等信息；

（4）study.optimize(objective, n_trials)：对objective函数里定义的超参进行搜索；

**3）optuna支持很多种搜索方式：**

（1）trial.suggest_categorical('optimizer', ['MomentumSGD', 'Adam'])：表示从SGD和adam里选一个使用；

（2）trial.suggest_int('num_layers', 1, 3)：从1～3范围内的int里选；

（3）trial.suggest_uniform('dropout_rate', 0.0, 1.0)：从0～1内的uniform分布里选；

（4）trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)：从1e-5～1e-2的log uniform分布里选；

（5）trial.suggest_discrete_uniform('drop_path_rate', 0.0, 1.0, 0.1)：从0～1且step为0.1的离散uniform分布里选；

## 应用

在WSOL代码中使用 optuna 进行超参数选择，首先构建 `objective` 函数：

```python
def objective(trail):
    bg_grid_size = trail.suggest_uniform('bg_grid_size', 0.0, 1.0)
    trainer = Trainer()
    trainer.args.bg_grid_size = bg_grid_size
    for epoch in range(trainer.args.epochs):
        print("Start epoch {} ...".format(epoch + 1))
        trainer.adjust_learning_rate(epoch + 1)   
        train_performance = trainer.train(split='train')
        trainer.report_train(train_performance, epoch + 1, split='train')
        print("Epoch {} done.".format(epoch + 1))
    trainer.evaluate(epoch + 1, split='val')
    trainer.print_performances()
    trainer.report(epoch + 1, split='val')
    trainer.save_checkpoint(epoch + 1, split='val')
    loc = trainer.performance_meters['val']['localization'].current_value
    return loc
```

每一次对超参数进行选择后都会调用执行 `objective` 函数，在该函数中首先选择一次超参数，然后对模型进行训练，训练结束后进行评价 `evaluate` 然后得到需要的指标` `loc` 进行返回。

```python
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=20)
print("Number of finished trials: ", len(study.trials))

print("Best trial:")
trial = study.best_trial

print("  Value: ", trial.value)

print("  Params: ")
for key, value in trial.params.items():
    print("    {}: {}".format(key, value))
```

主函数中创建 `study` 对象，然后使用 `optimize` 函数进行优化，试验次数是20次，之后选择最佳的参数值进行返回。











