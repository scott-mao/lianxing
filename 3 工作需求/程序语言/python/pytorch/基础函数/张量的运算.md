# 张量的拼接与拆分

## ``torch.cat``

该函数在在指定``dim``对函数进行拼接，拼接之后不会增加新的维度，是原有维度相加

```python
x = torch.randn(2,3)
y = torch.randn(3,3)
z = torch.cat((x,y),0)
print(z.size())
z为(5,3)
```

## ``torch.stack``

创建新的维度并在该指定维度上进行拼接

```
x = torch.randn(2,3)
y = torch.randn(2,3)
z = torch.stack((x,y),0)
print(z.size())
print(z)
此z为(2,2,3)
```

# 张量尺寸改变

```x.view()```



# 张量的复制

## `tensor.clone()`

返回tensor的拷贝，返回的新tensor和原来的tensor具有同样的大小和数据类型，不会共享内存。但是如果原来tensor的`require_grad=True`，那么新的tensor的梯度会流向原来的tensor。并且新的tensor作为中间节点其梯度不会保留，在被计算后就被释放。

```python
a = torch.tensor(2.,requires_grad=True)
f = 3*a
f.backward() 
print(a.grad)  #输出f对a的梯度值 3
a_clone = a.clone()#构建a的clone
f_2 = 2*a_clone
f_2.backward()
print(a_clone.grad)#由于a_clone是中间节点，所以梯度值不会被保存
print(a.grad)#a_clone的梯度值会叠加在其上面 3+2=5
#id函数返回对象的标识符，两者不是一个对象
print(id(a))				#140427767726144
print(id(a_clone))	#140427770997568
#tensor.data_ptr()返回tensor的内存地址，两者内存地址不通过
print(a.data_ptr())				#140427763328896
print(a_clone.data_ptr())	#140427787359808
```

## `tensor.detach()`

同样是原来tensor的拷贝，但是从原来的计算图脱离出来，不涉及梯度计算。但是与原来的tensor共享内存。修改其中一个tensor的值，另一个也会改变，因为是共享同一块内存，但如果对其中一个tensor执行某些内置操作，则会报错，例如resize\_、resize_as\_、set\_、transpose\_。

```python
		a =torch.tensor(2.,requires_grad=True)
    a_detach = a.detach()
    print("id(a):",id(a),"id(a_detach):",id(a_detach))
    #id(a): 140622609912512 id(a_detach): 140622609912640
    print("a的内存地址：",a.data_ptr(),"a_detach的内存地址：",a_detach.data_ptr()) 
    #a的内存地址： 140622619874624 a_detach的内存地址： 140622619874624
    f = 3 * a
    f.backward()
    print(a.grad) # 3.
    g = 4 * a_detach
    # g.backward()
    # print(a_detach.grad) 
    #RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn 说明a_detach没有梯度require_grad
    a = torch.tensor([4.,2.],requires_grad=True)
    a_detach = a.detach()
    a_detach[0] = 10
    print(a_detach) #[10.,2.]
    print(a) #[10.,2.]

```

## **`tensor.clone().detach() tensor.detach().clone()`**

两者的结果相同，返回的tensor和原来的tensor在数据和梯度上都没与任何关系























